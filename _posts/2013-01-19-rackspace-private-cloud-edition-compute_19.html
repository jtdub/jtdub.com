---
layout: post
title: Rackspace Private Cloud Edition - Compute Setup
date: '2013-01-19T18:48:00.001-06:00'
author: jtdub
tags:
- Openstack
- Private Cloud Edition
- Rackspace
modified_time: '2019-04-11T13:41:55.293-05:00'
thumbnail: https://4.bp.blogspot.com/-O911ci2E0rw/XK-HyiNSlpI/AAAAAAAA2Eg/Y3C_qIUGQD0-1JjEjXhoOUC4Wubbwja2wCLcBGAs/s72-c/20130118_011938.jpg
blogger_id: tag:blogger.com,1999:blog-2200496390325245811.post-8161979983127712904
blogger_orig_url: https://www.packetgeek.net/2013/01/rackspace-private-cloud-edition-compute_19.html
---

I finally got a chance to sit down and play with pre-built Open Stack 'Private Cloud Edition' built by Rackspace. Once it's installed, you can spin up instances right out of the box, but there are a few nuances to getting a functional platform for remote access and serving. I figured that I'd do a run through of the install and the initial changes that I made to get my install working.<br /><br />The first thing that you need to do is obtain the Private Cloud Edition (PCE) iso. The iso can be downloaded for FREE at the Rackspace website -<a href="http://www.rackspace.com/cloud/private/" target="_blank"> http://www.rackspace.com/cloud/private/.</a> Once it's downloaded and you have a bootable thumb drive or DVD, you're ready to rock! The system requirements for Installing PCE on the Rackspace website are pretty stout. They list the controller node as needing 16 GB of RAM, 144 GB of disk space, and a dual socket CPU with dual cores or a single quad core. Then they list the compute node as needing the same specs with the exception of RAM, which they list as 32 GB. Those are more of recommendations. I installed the compute and controller node (all-in-one) on a single desktop PC with a single dual core CPU, 4 GB of RAM, and a 80 GB hard drive. For testing purposes, this is completely fine. The requirement that is needed is that your CPU's will need to support virtualization technologies (VT-x), as the underlying hypervisor runs on KVM.<br /><br />The install is a pretty painless process. The first screen prompts you for a EULA, then how what you want to install - Controller, Compute, or All-in-One.<br /><br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="https://4.bp.blogspot.com/-O911ci2E0rw/XK-HyiNSlpI/AAAAAAAA2Eg/Y3C_qIUGQD0-1JjEjXhoOUC4Wubbwja2wCLcBGAs/s1600/20130118_011938.jpg" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="500" data-original-width="1600" height="100" src="https://4.bp.blogspot.com/-O911ci2E0rw/XK-HyiNSlpI/AAAAAAAA2Eg/Y3C_qIUGQD0-1JjEjXhoOUC4Wubbwja2wCLcBGAs/s320/20130118_011938.jpg" width="320" /></a></div><br /><br /><br />After that, you set up the IP Address of the server, the netblock assigned to VM's, (the default is 172.31.0.0/24 - I left this default), and the user accounts (admin Open Stack account, Open Stack user account, and server local user account). After that it's a matter of the automated installer installing the packages. Once it's done installing, it will boot up and you'll be ready to play!<br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="https://3.bp.blogspot.com/-VmD9L-o9s7U/XK-H6CW7xyI/AAAAAAAA2Ek/phxVBP-0HIYkQz_VTuUTBfs3eJP6t_HIgCLcBGAs/s1600/20130118_152157.jpg" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="499" data-original-width="1600" height="99" src="https://3.bp.blogspot.com/-VmD9L-o9s7U/XK-H6CW7xyI/AAAAAAAA2Ek/phxVBP-0HIYkQz_VTuUTBfs3eJP6t_HIgCLcBGAs/s320/20130118_152157.jpg" width="320" /></a></div><br /><br /><br /><br />If you've ever used the Rackspace public cloud, you will notice that the UI looks very familiar. Though, if you prefer, the UI can be changed to the default Open Stack UI. The first thing that we'll want to do when we log in is to grab your API credentials, so that you can easily use the command line tools. To do this, log in with your admin account, select the 'Settings' link at the top right of the screen, then select the "OpenStack API" tab, select 'admin' as the project, and finally press the "Download RC File" button.<br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="https://4.bp.blogspot.com/-HKSins2sMj0/XK-IFzr2ZbI/AAAAAAAA2Es/8-tutcO2ZBg2MxJNlv7Op5u19cO4O_BzgCLcBGAs/s1600/Screenshot-from-2013-01-19-222937.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="689" data-original-width="689" height="320" src="https://4.bp.blogspot.com/-HKSins2sMj0/XK-IFzr2ZbI/AAAAAAAA2Es/8-tutcO2ZBg2MxJNlv7Op5u19cO4O_BzgCLcBGAs/s320/Screenshot-from-2013-01-19-222937.png" width="320" /></a></div><br />Once the openrc.sh file is downloaded, you can copy it to your PCE server so that we can begin configuring what needs to be configured from the command line. As you can see below, I used scp to copy the file to the server.<br /><pre class="lang:default decode:true">[jtdub@jtdub-desktop Downloads]$ scp openrc.sh james@172.16.2.254:/home/james/.novarc<br />james@172.16.2.254's password: <br />openrc.sh                                                                                                                                              100%  958     0.9KB/s   00:00</pre><br />After the file is copied to the server, we'll ssh to the server and use the CLI tools to configure floating IP Addresses. The one thing that I've noticed while playing around with Open Stack is that, at least in my limited experience, the 'nova-manage' and chef commands will not properly execute unless you have administrative privileges on the server, therefore, I generally go ahead and 'sudo su -' to root while I'm using the 'nova-manage' and chef commands, but will continue to utilize my non-privileged account for using the 'nova' command.<br /><br />So, let's add a floating IP Address range. My PCE All-in-One server is currently sitting on the 172.16.2.0/24 network. 172.16.2.1 is the router, 172.16.2.254 is the PCE server, and there is another computer on 172.16.2.12. I want to add a range of addresses that will be on the 172.16.2.0/24 network, but will not conflict with existing hosts. For testing purposes, I do not need a large number of addresses, so I decided to carve out a section of my 172.16.2.0/24 network to assign as floating IP Addresses to instances that spin up. In my case, I only need about 16 addresses, so I chose to use the prefix of 172.16.2.32/28. That will tell Open Stack to assign addresses 172.16.2.33 - 46 to VM instances as they spin up and will re-claim those addresses as the instances are torn down. This allows me to continue to utilize the 172.16.2.0/24 network without conflict.<br /><pre class="lang:default decode:true">james@openstack:~$ sudo su -<br />root@openstack:~# source /home/james/.novarc <br />Please enter your OpenStack Password: <br />root@openstack:~# nova-manage floating create --pool=172.16.2.32-net --ip_range=172.16.2.32/28<br />2013-01-19 23:49:15 DEBUG nova.utils [req-99d8cbf4-8821-4c3d-afc7-9a584cfc1748 None None] backend &lt;module 'nova.db.sqlalchemy.api' from '/usr/lib/python2.7/dist-packages/nova/db/sqlalchemy/api.pyc'&gt; __get_backend /usr/lib/python2.7/dist-packages/nova/utils.py:502<br />root@openstack:~# nova-manage floating list<br />2013-01-19 23:49:22 DEBUG nova.utils [req-034aa938-a81f-428b-be81-96895607bb4c None None] backend &lt;module 'nova.db.sqlalchemy.api' from '/usr/lib/python2.7/dist-packages/nova/db/sqlalchemy/api.pyc'&gt; __get_backend /usr/lib/python2.7/dist-packages/nova/utils.py:502<br />None 172.16.2.33 None 172.16.2.32-net br0<br />None 172.16.2.34 None 172.16.2.32-net br0<br />None 172.16.2.35 None 172.16.2.32-net br0<br />None 172.16.2.36 None 172.16.2.32-net br0<br />None 172.16.2.37 None 172.16.2.32-net br0<br />None 172.16.2.38 None 172.16.2.32-net br0<br />None 172.16.2.39 None 172.16.2.32-net br0<br />None 172.16.2.40 None 172.16.2.32-net br0<br />None 172.16.2.41 None 172.16.2.32-net br0<br />None 172.16.2.42 None 172.16.2.32-net br0<br />None 172.16.2.43 None 172.16.2.32-net br0<br />None 172.16.2.44 None 172.16.2.32-net br0<br />None 172.16.2.45 None 172.16.2.32-net br0<br />None 172.16.2.46 None 172.16.2.32-net br0</pre><br />At this point, I should spend a little bit of time to describe how the networking for VM instances is going to work. When we initially installed PCE, we were prompted with a screen asking us for a CIDR block for Nova fixed (VM) networking. The default is 172.31.0.0/24<br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="https://4.bp.blogspot.com/-ULbb7bT6uZ8/XK-IV7ZSo7I/AAAAAAAA2E0/ogJ6hGZR1Lck59Mizy36lch1saKQj3w0QCLcBGAs/s1600/20130118_012148.jpg" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="583" data-original-width="1600" height="116" src="https://4.bp.blogspot.com/-ULbb7bT6uZ8/XK-IV7ZSo7I/AAAAAAAA2E0/ogJ6hGZR1Lck59Mizy36lch1saKQj3w0QCLcBGAs/s320/20130118_012148.jpg" width="320" /></a></div><br />One IP Address on the 172.31.0.0/24 will be allocated to the br0 interface of your PCE server and the remaining will be assigned to your instances as they boot up. The br0 interface will also contain the IP Address of your PCE server. In this case, that IP Address will be 172.16.2.254.<br /><pre class="lang:default decode:true">root@openstack:~# ip addr show br0<br />3: br0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP <br />    link/ether 00:23:ae:90:aa:7c brd ff:ff:ff:ff:ff:ff<br />    inet 172.31.0.5/24 brd 172.31.0.255 scope global br0<br />    inet 172.16.2.254/24 brd 172.16.2.255 scope global br0<br />    inet6 fe80::4858:14ff:fe72:7112/64 scope link <br />       valid_lft forever preferred_lft forever<br />root@openstack:~# route -n<br />Kernel IP routing table<br />Destination     Gateway         Genmask         Flags Metric Ref    Use Iface<br />0.0.0.0         172.16.2.1      0.0.0.0         UG    100    0        0 br0<br />169.254.123.0   0.0.0.0         255.255.255.0   U     0      0        0 chefbr0<br />172.16.2.0      0.0.0.0         255.255.255.0   U     0      0        0 br0<br />172.31.0.0      0.0.0.0         255.255.255.0   U     0      0        0 br0</pre><br />The br0 interface is a bridge interface that connects the VM network to the physical interface, eth0. Open Stack then routes (layer 3) traffic coming from eth0 to the 172.31.0.0/24 network. It also uses iptables to create a PAT/NAT, so that the instances can communicate on the network, and the internet if you allow it. However, computers outside the PCE environment can't communicate with the VM instances directly, because those computers will be unaware of the 172.31.0.0/24 network. This is where the floating IP Addresses come into play. The floating IP Addresses create a one-to-one NAT mapping a VM instance to an address in your floating IP Address range. In this case, my floating IP Address range is 172.16.2.32/28. Also, by default, the PCE iptables rules are very restrictive and don't allow incoming traffic to communicate with the VM instances. To allow this traffic, you will have to create or edit security groups. This will come later on. Below is a diagram of the PCE network environment.<br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="https://3.bp.blogspot.com/-b3DfJqIJj-8/XK-IfrqjagI/AAAAAAAA2E8/HixYH2vF4yAKeKiFT-yO4yRpN8MFGlyLACLcBGAs/s1600/OpenStack-network.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="264" data-original-width="830" height="101" src="https://3.bp.blogspot.com/-b3DfJqIJj-8/XK-IfrqjagI/AAAAAAAA2E8/HixYH2vF4yAKeKiFT-yO4yRpN8MFGlyLACLcBGAs/s320/OpenStack-network.png" width="320" /></a></div><br />Now that we understand how the networking works, let's log back into the UI as a normal user. As the normal user, we're going to edit our default security group to define what traffic we want to allow to our VM instances, we'll add a couple floating IP Addresses to our project, create keypairs that will be used to allow us to access our VM, we'll add a pre-built Fedora 17 image to our default images, and finally, we'll spin up an instance and verify that we can access it from an outside computer.<br /><br />Once we login as our normal user, the first thing that we'll do is edit our default security group to define what traffic that we want to allow to our VM instances from the outside world by default. In my test, I am going to allow ICMP echo (code -1, type -1), all UDP traffic, and all TCP traffic. To access the security groups, select the "Access &amp; Security" tab along the top menu, locate the "Security Groups" section, and press the "Edit Rules" on the default group.<br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-ttze5rk9FRE/XK-I9TNHBEI/AAAAAAAA2FI/sM8p_Nq1vdgqr8tMWNcE-g43jrLyyWqqACLcBGAs/s1600/Screenshot-from-2013-01-19-235232.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="538" data-original-width="1064" height="161" src="https://1.bp.blogspot.com/-ttze5rk9FRE/XK-I9TNHBEI/AAAAAAAA2FI/sM8p_Nq1vdgqr8tMWNcE-g43jrLyyWqqACLcBGAs/s320/Screenshot-from-2013-01-19-235232.png" width="320" /></a></div><br />Once you've located that screen, enter your rules.<br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="https://3.bp.blogspot.com/-wr5IpMKfL44/XK-JGqomI4I/AAAAAAAA2FM/f8cK2dfh39QD-KwZVtj4FMq_ApaGGJPQACLcBGAs/s1600/Screenshot-from-2013-01-19-235518.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="543" data-original-width="565" height="307" src="https://3.bp.blogspot.com/-wr5IpMKfL44/XK-JGqomI4I/AAAAAAAA2FM/f8cK2dfh39QD-KwZVtj4FMq_ApaGGJPQACLcBGAs/s320/Screenshot-from-2013-01-19-235518.png" width="320" /></a></div><br />Now that the security group has been edited, we'll go ahead and add floating IP Addresses, which are on the same "Access &amp; Security" page. To do this, press the "Allocate IP To Project" button, select your pool, if you have multiple IP Address pools, and press the&nbsp; "Allocate IP" button. You can add as many IP Addresses as you need for your project. By default, there are quotas in place that limits a "project" to 10 floating IP Addresses. This quota can be changed and will be discussed later on.<br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-W0RwHE1GemU/XK-JPb_F7MI/AAAAAAAA2FU/YtVVPEm_MPwikMHB5suXP3tgRCyuFouXQCLcBGAs/s1600/Screenshot-from-2013-01-19-235954.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="279" data-original-width="564" height="158" src="https://1.bp.blogspot.com/-W0RwHE1GemU/XK-JPb_F7MI/AAAAAAAA2FU/YtVVPEm_MPwikMHB5suXP3tgRCyuFouXQCLcBGAs/s320/Screenshot-from-2013-01-19-235954.png" width="320" /></a></div><br />Lastly, on the same "Access &amp; Security" page let's generate encryption key pairs that will be used to access our VM instances. In the "Keypairs" section, press the "Create Keypair" button. This will bring up a screen that will allow you to name the key pair.<br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="https://4.bp.blogspot.com/-zXeKeT0YbC0/XK-JXOulW9I/AAAAAAAA2Fc/Q_kQqgE5Dxw8sgZAXpiVurCAf3VM4WkswCLcBGAs/s1600/Screenshot-from-2013-01-20-000522.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="291" data-original-width="564" height="165" src="https://4.bp.blogspot.com/-zXeKeT0YbC0/XK-JXOulW9I/AAAAAAAA2Fc/Q_kQqgE5Dxw8sgZAXpiVurCAf3VM4WkswCLcBGAs/s320/Screenshot-from-2013-01-20-000522.png" width="320" /></a></div><br /><br />Once the keypair has been generated, you'll be prompted to download a pem file. Do so, and then add the keypair to your keys for ssh. In Linux, you use the ssh-add command. As this is a private key, you won't want any other users to be able to read the key, so be sure to change the permissions on the file so that only your account can access the file.<br /><pre class="lang:default decode:true">[jtdub@jtdub-desktop Downloads]$ chmod 600 jtdub-keypair.pem <br />[jtdub@jtdub-desktop Downloads]$ ssh-add jtdub-keypair.pem <br />Identity added: jtdub-keypair.pem (jtdub-keypair.pem)</pre><br />We're at the light at the end of the tunnel! If you wanted to, you could now just spin up VM instances utilizing the default images that come with PCE. However, I'm going to download a pre-built image of Fedora 17, so that I can demonstrate how to import images. The Fedora 17 image that I'm going to use can be downloaded at <a href="http://berrange.fedorapeople.org/images/" target="_blank">http://berrange.fedorapeople.org/images/</a>. In your UI, still logged in as the unprivileged user, select the "Images &amp; Snapshots" tab. Once there, select the "Create Image" button, fill out the information on the form, and press the "Create Image" button.<br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-8Vb7lven1f4/XK-Jgcrl05I/AAAAAAAA2Fk/VwP7J-2Jt3Mrt56LuCTQeprCdE8OaeqVwCLcBGAs/s1600/Screenshot-from-2013-01-20-001658.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="425" data-original-width="571" height="237" src="https://1.bp.blogspot.com/-8Vb7lven1f4/XK-Jgcrl05I/AAAAAAAA2Fk/VwP7J-2Jt3Mrt56LuCTQeprCdE8OaeqVwCLcBGAs/s320/Screenshot-from-2013-01-20-001658.png" width="320" /></a></div>&nbsp; <br />Now, you just wait for the image to download. It will take a little while depending on the speed of your Internet connection, as well as the size of the image.<br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-LM0TYAaXRP4/XK-JpU8j7aI/AAAAAAAA2Fs/qAC5wUcSB8wV_g7nYAB-pZ1chSGDyozaQCLcBGAs/s1600/Screenshot-from-2013-01-20-001716.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="417" data-original-width="1035" height="128" src="https://1.bp.blogspot.com/-LM0TYAaXRP4/XK-JpU8j7aI/AAAAAAAA2Fs/qAC5wUcSB8wV_g7nYAB-pZ1chSGDyozaQCLcBGAs/s320/Screenshot-from-2013-01-20-001716.png" width="320" /></a></div><br />When the image download completes, we can finally create our first instance. This can be accomplished from the "Instances" tab, in the UI, by pressing the "Launch Instance" button. On the Launch Instance page there are several options. It's best to spend a few minutes to get familiar with the options. I'll give a run down of the settings that I used.<br /><ul><br /><li>Details Tab:<br /><ul><br /><li>"Image", I selected my newly minted fedora17-image.</li><br /><li>"Instance Name", I chose the name f17-test</li><br /><li>"Flavor", I left a m1.tiny (512MB / RAM) instance.</li></ul></li><br /><li>Access &amp; Security Tab:<br /><ul><br /><li>"Keypair", I chose my jtdub-keypair</li></ul></li></ul><br />After that, I pressed the "Launch" button. In no time flat, my first instance was up and running. The only other thing that I need to do is associate a floating IP Address to the VM instance.<br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-H0OzLIyL6Lg/XK-J0LW6aFI/AAAAAAAA2F0/zLoGbHFYsdEfsF8nXa6nH_f0GF-2hjYxgCLcBGAs/s1600/Screenshot-from-2013-01-20-002907.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="423" data-original-width="564" height="240" src="https://1.bp.blogspot.com/-H0OzLIyL6Lg/XK-J0LW6aFI/AAAAAAAA2F0/zLoGbHFYsdEfsF8nXa6nH_f0GF-2hjYxgCLcBGAs/s320/Screenshot-from-2013-01-20-002907.png" width="320" /></a><a href="https://1.bp.blogspot.com/-2skbUQ2fpwc/XK-J7i0EoII/AAAAAAAA2F8/1GuNv0eExPA7FhF2496Iqx9cBU_xFzzuQCLcBGAs/s1600/Screenshot-from-2013-01-20-003019.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="322" data-original-width="564" height="182" src="https://1.bp.blogspot.com/-2skbUQ2fpwc/XK-J7i0EoII/AAAAAAAA2F8/1GuNv0eExPA7FhF2496Iqx9cBU_xFzzuQCLcBGAs/s320/Screenshot-from-2013-01-20-003019.png" width="320" /></a></div><br /><br />To associate a floating IP Address to an instance, locate your VM Instance on the "Instances" page, drop down the menu on the "Create Snapshot" button, and select "Associate Floating IP". Once the "Manage Floating IP Associations" page pulls up, select and IP Address and press the "Associate" button.<br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="https://3.bp.blogspot.com/-YX7i6mNJ4_o/XK-KOKAA-wI/AAAAAAAA2GI/fqLJfK5AXTQGo64SPjoXIcQjxxeAXLJ3gCLcBGAs/s1600/Screenshot-from-2013-01-20-003516.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="360" data-original-width="1090" height="105" src="https://3.bp.blogspot.com/-YX7i6mNJ4_o/XK-KOKAA-wI/AAAAAAAA2GI/fqLJfK5AXTQGo64SPjoXIcQjxxeAXLJ3gCLcBGAs/s320/Screenshot-from-2013-01-20-003516.png" width="320" /></a></div><div class="separator" style="clear: both; text-align: center;"><a href="https://2.bp.blogspot.com/-Me-7qeGu7Do/XK-KVEdz94I/AAAAAAAA2GM/lKf-aV0OIsUrl2z0YeRvoRyCG6TKmGtggCLcBGAs/s1600/Screenshot-from-2013-01-20-003632.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="342" data-original-width="563" height="194" src="https://2.bp.blogspot.com/-Me-7qeGu7Do/XK-KVEdz94I/AAAAAAAA2GM/lKf-aV0OIsUrl2z0YeRvoRyCG6TKmGtggCLcBGAs/s320/Screenshot-from-2013-01-20-003632.png" width="320" /></a></div><br />That's it! The first instance is up and running and should be remotely accessible! To test it, I'll ssh to the instance.<br /><pre class="lang:default decode:true">[jtdub@jtdub-desktop ~]$ ping -c2 172.16.2.33<br />PING 172.16.2.33 (172.16.2.33) 56(84) bytes of data.<br />64 bytes from 172.16.2.33: icmp_seq=1 ttl=62 time=0.870 ms<br />64 bytes from 172.16.2.33: icmp_seq=2 ttl=62 time=0.801 ms<br /><br />--- 172.16.2.33 ping statistics ---<br />2 packets transmitted, 2 received, 0% packet loss, time 1000ms<br />rtt min/avg/max/mdev = 0.801/0.835/0.870/0.045 ms<br />[jtdub@jtdub-desktop ~]$ ssh -l root 172.16.2.33<br />The authenticity of host '172.16.2.33 (172.16.2.33)' can't be established.<br />RSA key fingerprint is 3d:ec:47:85:9c:72:9b:3c:87:b6:0a:25:fa:7d:0b:d9.<br />Are you sure you want to continue connecting (yes/no)? yes<br />Warning: Permanently added '172.16.2.33' (RSA) to the list of known hosts.<br />[root@f17-test ~]# ip addr show<br />1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 16436 qdisc noqueue state UNKNOWN <br />    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00<br />    inet 127.0.0.1/8 scope host lo<br />    inet6 ::1/128 scope host <br />       valid_lft forever preferred_lft forever<br />2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000<br />    link/ether fa:16:3e:77:9d:16 brd ff:ff:ff:ff:ff:ff<br />    inet 172.31.0.2/24 brd 172.31.0.255 scope global eth0<br />    inet6 fe80::f816:3eff:fe77:9d16/64 scope link tentative dadfailed <br />       valid_lft forever preferred_lft forever</pre><br />That's it! We now have a PCE compute cloud running. Whew! LONG blog! So for now, I'll wrap this up. Soon, I'll create another much shorter blog to show how to modify the UI back to the default Open Stack UI, if you prefer. In that same blog, I'll also talk about project quotas and how to modify them. That's it for now! Thanks for reading.<br /><br />Documentation References:<br /><ul><br /><li><a href="http://docs.openstack.org/trunk/openstack-compute/admin/content/ch_getting-started-with-openstack.html" target="_blank">http://docs.openstack.org/trunk/openstack-compute/admin/content/ch_getting-started-with-openstack.html</a></li><br /><li><a href="http://www.rackspace.com/knowledge_center/getting-started/rackspace-private-cloud" target="_blank">http://www.rackspace.com/knowledge_center/getting-started/rackspace-private-cloud</a></li></ul>